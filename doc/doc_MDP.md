# MDP

## 実践

[この記事](https://qiita.com/Hironsan/items/56f6c0b2f4cfd28dd906)を参考に、MDPの学習を行います。

### はじめに

**MDP**（Markov Decision Process; マルコフ決定過程）はエージェントの行動と環境の変化をモデル化するための枠組みです。MDPにおいてエージェントは現在の行動と状態を**完全に**知ることができ、エージェントの行動に対して環境は**確率的に**推移します。

強化学習の目的はエージェントが獲得する報酬を最大化するための方策（*policy*）を学習することです。ここで、任意の環境（状態）を与えられたときに、最終的に受け取る報酬を最大化することのできる行動を選択できるような状態-行動間の対応が、最適な方策といえます。以下では（有限）MDPの理論と、それらのPythonによる実装を記述します。

### 問題設定

以下のような環境を設定し、これをMDPで実装することを考えます。環境の設定は[このサイト](https://qiita.com/Hironsan/items/56f6c0b2f4cfd28dd906#問題設定)に習います。

![img](imgs/extimgs/doc_MDP_img1.webp)

エージェントは白いグリッド内を動くことができますが、10%の確率で想定方向の左に、10%の確率で右に動いてしまいます。ただし、動作方向に壁があった場合は動きません。各時刻に報酬を受け取り、ゴールやトラップにたどり着いたときにゲームを終了します。

以下ではこの問題を**迷路問題**と呼称することにします。

### 理論

離散時間の確率過程である（有限）MDPは4つの要素の組 $(\mathcal{S, A}, p_T, r)$ により規定されます。ここで、その要素を

$$\begin{aligned}
\mathcal{S}=\{s_1,s_2,...,s_N\}&:状態の有限集合\\
\mathcal{A}=\{a_1,a_2,...,a_M\}&:行動の有限集合\\
p_T&:状態遷移関数\\
r&:報酬関数（即時報酬）\\
\end{aligned}$$

とします。それぞれ、

|名称|説明|迷路問題の場合|
|-|-|-|
|状態集合 $\mathcal S$ |環境が取りうる状態 $s$ の集合|エージェントがどのグリッドにいるか|
|行動集合 $\mathcal A$ |取りうる行動 $a$ の集合|上下左右への移動|
|状態遷移関数 $p_T$ | 時刻 $t$ 、状態 $s$ における行動 $a$ が、次ステップで状態を $s'$ に遷移させる確率<br> $p_T=\mathrm{Pr}(s_{t+1}=s'\|s_t=s,a_t=a)$ |例）右への移動を選んだとき<br>　80%の確率で右に移動<br>　20%の確率で上下に移動する（壁方向への移動を除く）|
|報酬関数 $r$ |行動 $a$ により状態が $s$ から $s'$ へ遷移した場合の即時報酬<br> $r=R_a(s,s'), R_a(s),...$ |後述|

です。今回の場合、累積報酬を最大化する行動戦略を学習することを目的とします。この行動戦略のことを**方策**（*policy*; ポリシー）と呼びます。

#### 方策（ポリシー）

上に述べたように、MDPのゴールはエージェントにとって最適な方策 $\pi$ を探すことです。ここで、方策 $\pi$ は

$$\pi=\pi(s)\space\mathrm{s.t.}\space\pi\colon\mathcal{S}\to\mathcal{A}$$

で定義される、行動 $s$ の関数です。最適な方策 $\pi^*$ を目指して学習は行われ、よい方策であるほど目的が達成されやすくなります。

迷路問題の場合、最適方策 $\pi^*$ に近い方策を用いるほど、エージェントはゴールすることが多くなり、また早くなると考えられます。

#### 報酬

上に述べたように、方策 $\pi$ を選択する目的は累積報酬を最大化することです。ここで、累積報酬は単に各ステップの報酬を足しただけの $\sum_{t=0}^\infty{r}$ ではなく、

$$\sum_{t=0}^\infty{\gamma^tr}$$

で計算されます。ここで、上の累積報酬は各ステップの行動 $a_t$ が方策に従い $a_t=\pi(s_t)$ 選択された場合に計算されるものです。また $\gamma\space(0\leq\gamma\leq 1)$ は割引係数とよばれ、小さいほどエージェントが将来得られる報酬より現在得られる報酬を重視するようになります。

このように計算された累積報酬は割引報酬とよばれ、単純な累積報酬と比較して値が発散しないなど利点があります。

ここまででMDPというモデルの概要はわかりましたが、最適な方策を見つける方法を知らなくてはどうしようもありません。したがって、いくつかの最適な方策を見つけるアルゴリズム（価値反復法、Q-Learning）を説明します。

#### プランニング方法１（価値反復法）



## 参考文献

[1] [Pythonではじめる強化学習](https://qiita.com/Hironsan/items/56f6c0b2f4cfd28dd906)

[2] [Markov Decision Process](https://en.wikipedia.org/wiki/Markov_decision_process)