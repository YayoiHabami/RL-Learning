# 準備

## はじめに

強化学習は機械学習の一種であり、報酬という概念が出てくるという点、その期待値を最大化するような逐次的意思決定ルールを学習することを目的とする点が特徴的です。ここで、逐次的意思決定ルールは**方策**（*policy*）と呼ばれます。また一般に、強化学習では**マルコフ性**という仮定をおきます。これらについて説明を行っていきます。

## 目次

- [準備](#準備)
  - [はじめに](#はじめに)
  - [目次](#目次)
  - [逐次的意思決定問題](#逐次的意思決定問題)
    - [マルコフ性](#マルコフ性)
    - [マルコフ決定過程](#マルコフ決定過程)
  - [方策](#方策)
    - [方策の分類 (not ok)](#方策の分類-not-ok)
  - [定式化](#定式化)
    - [概観：逐次的意思決定問題](#概観逐次的意思決定問題)
    - [MDPの表現の統一](#mdpの表現の統一)
      - [エルゴード性](#エルゴード性)
    - [リターン（累積報酬）](#リターン累積報酬)
    - [目的関数](#目的関数)
  - [参考文献](#参考文献)


## 逐次的意思決定問題

強化学習が目的とする方策の最適化問題、すなわち**逐次的意思決定問題**（*sequential decision-making problem*）を解くために必要な概念について学習していきます。

### マルコフ性

何らかの事象 $A$ の確率を $\mathrm{Pr}(A)$ と書くとしましょう。取りうる値とその値になる確率が定められている変数は**確率変数**と呼ばれ、実際に取った値は**実現値**と呼ばれます。理想的な６面さいころを例に取ってこれを説明すると、

$$\mathrm{Pr}(X=x)=\frac16,\space\forall x\in\mathcal{X}\triangleq\{1,2,3,4,5,6\}$$

と書けます。ここで、以下では確率変数を大文字（ $X$ ）で、実現値を小文字（ $x$ ）で、確率変数の取る値の集合をカリグラフ体（ $\mathcal X$ ）を用いて区別することににします。このような確率変数と確率との対応関係のことを**確率分布**（*probability distribution*）と呼びます。

さて、**マルコフ性**ですが、確率変数の並びを考えた場合に、将来の確率変数の条件付確率分布が、現時点のステップ（ $t$ ）にのみ依存する（すなわち $t-1$ 以前の値 $x_1,...,x_{t-1}$ には依存しない）という性質です。マルコフ性を持つ場合、それらは

$$\mathrm{Pr}(X_{t+k}=x|X_s=x_s,\forall s\leq t)=\mathrm{Pr}(X_{t+k}=x|X_t=x_t)$$

を満たします。ここで $t,k$ は任意の自然数であり、また $x_s,x\in\mathcal{X}$ です。

> 確率変数の並び：確率変数の系列のことであり、**確率過程**と呼ばれます。

確率変数 $X$ を状態変数とみなせば、 $\mathrm{Pr}(X_{t+1}=x'|X_t=x)$ は状態 $x$ が次のステップで $x'$ に遷移する確率を表すので、一般に**状態遷移確率**（*state transition probability*）と呼ばれます。また、マルコフ性を持つ確率過程を **マルコフ過程** といい、状態変数の取りうる値が離散的である場合には**マルコフ連鎖**と呼ばれます。

### マルコフ決定過程

強化学習においては何らかの選択や行動を行いますから、その**行動**（action）と、その良し悪しを判断させるための**報酬**（reward）をマルコフ連鎖に加えます。このような過程を**マルコフ決定過程**（Markov decision process; MDP）と呼び、次の組 $(\mathcal{S,A},p_{s_0},p_T,g)$ で定義します[0]。

|名称|定義|
|:-:|-|
|有限状態集合| $\mathcal{S}\triangleq\{s^1,...,s^{\|\mathcal{S}\|}\}\ni s$ |
|有限行動集合| $\mathcal{A}\triangleq\{a^1,...,a^{\|\mathcal{A}\|}\}\ni a$ |
|初期状態確率関数| $p_{s_0}\colon\mathcal{S}\to[0,1]\colon p_{s_0}(s)\triangleq\mathrm{Pr}(S_0=s)$ |
|状態遷移確率関数| $p_T\colon\mathcal{S\times S\times A}\to[0,1]\colon$ |
|〃| $p_T(s'\|s,a)\triangleq\mathrm{Pr}(S_{t+1}=s'\|S_t=s,A_t=a),\space\forall t\in\mathbb{N}_0$ |
|報酬関数| $g\colon\mathcal{S\times A}\to\mathbb{R}$ |

> $\mathbb{N}_0 \coloneqq \mathbb{N}\cup\bold{0}$ 、 $|\mathcal{X}|$ は $\mathcal X$ の要素数としました。

ここで、確率変数 $S_t$ と $A_t$ は時間ステップ $t\in\mathbb{N}_0$ での状態変数と行動変数を表します。上からわかるように、このMDPは「有限状態集合、有限行動集合の離散時間MDP」です。

定義から、報酬関数 $g$ は有界関数（ $\infty$ に飛ばない）であり、

$$|g(s,a)|\leq R_{max},\space\forall (s,a)\in\mathcal{S\times A}\tag 1$$

を満たす $R_{max}\in\mathbb R$ が存在することを仮定していることになります。また、報酬の集合 $\mathcal R$ を次のように定義します。

$$\mathcal{R}\triangleq\{r\in\mathbb{R}\colon r=g(s,a),\exist (s,a)\in\mathcal {S,A}\}$$

ここで、報酬にマイナスを掛けたものを**損失**（cost）と呼ぶことがありますが、累積損失の最小化を目的とした問題は累積報酬の最大化を目的とした問題と同様であることに注意したいです。

次に、MDPにおいて行動を選択する基準である**方策**（*policy*）を定義します。方策は関数であり、ここでは現ステップの状態 $s にのみ依存して確率的に行動を選択する**確率的方策**を

$$\pi\colon\mathcal{A\times S}\to[0,1]\colon\pi(a|s)\triangleq\mathrm{Pr}(A=a|S=s)\tag2$$

のように定義します。ここで、方策 $\pi$ を含めたMDP; $M$ を、

$$M(\pi)\triangleq\{\mathcal{S,A},p_{s_0},p_T,g,\pi\}\tag3$$

と表記することにします。また（すべての）方策を含む方策集合 $\Pi$ を次のように定義します。

$$\Pi\triangleq\left\{\pi\colon\mathcal{A\times S}\to[0,1]\colon\sum_{a\in\mathcal A}\pi(a|s)=1,\space\forall s\in\mathcal S\right\}$$

最後に、マルコフ決定過程 $M(\pi)$ がどのようにステップを進めるかについて下記に示します。

> マルコフ決定過程 $M(\pi)=\{\mathcal{S,A},p_{s_0},p_T,g,\pi\}$
>
> 0. 時間ステップ $t=0$ で初期化を行い、（初期状態確率 $p_{s_0} に従い）初期状態 $s_t\space(\sim p_{s_0})$ を決定する
> 1. 状態 $s_t$ と 方策 $\pi(\cdot|s_t)$ から、行動 $a_t$ を選択する
> 2. 行動 $a_t$ に対する報酬 $r_t=g(s_t,a_t)$ を受け取る
> 3. 現在の状態 $s_t$ と行動 $a_t$ から状態遷移確率 $p_T(\cdot|s_t,a_t)$ により、状態を次の $s_{t+1}$ へ遷移させる
> 4. 時間ステップを $t$ から $t+1$ に進め、1. に戻る

> $s_t\sim p_{s_0}$ は確率変数 $s_t$ が確率分布 $p_{s_0}$ に従うことを意味します。

## 方策

上で述べたように、エージェントは方策に従い行動を決定します。この方策について、以下ではMDPに関するものを定義します。

> 一般的に機械学習の分野では、上記のような行動や選択を行う学習者のことを**エージェント**（*agent*）と呼びます。また、学習で扱う系全体のことは**環境**（*environment*）と呼びます。例として、家でハイハイを学習する赤ちゃんをこれに見立てれば、赤ちゃんはエージェントであり、その環境は家といえます。

### 方策の分類 (not ok)

式 $(2)$ で定義した確率的方策 $\pi$ の集合 $\Pi$ の部分集合として、**決定的方策**（*deterministic policy*） $\pi^d$ の集合 $\Pi^d$ を定義できます。

$$\Pi^d\triangleq\{\pi^d\colon\mathcal{S\to A}\}\tag5$$

また、

$$\pi(a|s)\coloneqq\left\{\begin{aligned}
&1\space&(a=\pi^d(s))\\
&0\space&(それ以外)
\end{aligned}\right.,\space\forall (s,a)\in\mathcal{S\times A}$$

のように $\pi^d$ を確率的方策 $\pi$ の形式に書き直すことができ、 $\Pi^d\subset\Pi$ がわかります。

上に定義した $\pi$ や $\pi^d$ は状態 $s$ にのみ依存し、過去の経験には独立であることから**マルコフ方策**（*Markov policy*）といいます。さらに、時間ステップ $t$ にも依存しないことから**定常なマルコフ方策**（*stationary Markov policy*）とも呼ばれます。一方、非定常な方策系列

$$\pmb{\pi}^m\triangleq\{\pi_0\in\Pi,\pi_1\in\Pi,...\}\in\Pi^M\tag6$$

や、時間不定の定常な方策の系列（式 $(2),(5)$ ）

$$\begin{align}
\pmb{\pi}^s&\triangleq\{\pi,\pi,...\}\in\Pi^S,&\pi\in\Pi\tag{7}\\
\pmb{\pi}^sd&\triangleq\{\pi^d,\pi^d,...\}\in\Pi^{SD},&\pi^d\in\Pi^d\tag{8}\\
\end{align}$$

を定義できます。

一方、現在の状態に加えてそれ以前の経験にも依存する非マルコフ方策も考えることができます。MDPにおいて、現在の時間ステップ $t$ までのすべての経験の履歴は

$$\{s_0,a_0,r_0,...,s_{t-1},a_{t-1},r_{t-1},s_t\}\triangleq h_t\in\mathcal{H}_t\tag{9}$$

で定義されます。これに基づき行動選択確率を決めるような履歴依存の方策

$$\begin{aligned}&\pi_t^h\colon\mathcal{A}\times\mathcal{H}_t\to[0,1],\\
&\pi_t^h(a|h_t)\triangleq\mathrm{Pr}(A=a,|H_t=h_t)\end{aligned}\tag{10}$$

も定義できます。任意の $\pi_t^h$ を含む方策集合を

$$\Pi_t^h\triangleq\left\{\pi_t^h\colon\mathcal{A}\times\mathcal{H}_t\to[0,1]\colon\sum_{a\in\mathcal{A}}{\pi_t^h(a|h_t)=1}\right\}\tag{11}$$

と表記し、その系列を

$$\pmb{\pi}^h\triangleq\{\pi_0^h,\pi_1^h,...\}\in\pmb{\Pi}^H\triangleq(\Pi_t^h)_{t\in\mathbb{N}_0}\tag{12}$$

とします。以上の系列は、下記のような包含関係を持ちます。

$$\pmb{\Pi}^{SD}\subseteq\pmb{\Pi}^S\subseteq\pmb{\Pi}^M\subseteq\pmb{\Pi}^H$$

```mermaid
graph TD;
  subgraph A["履歴依存の方策系列の集合"]
    B
  end
  subgraph B["(非定常な)マルコフ方策系列の集合"]
    C
  end
  subgraph C["定常なマルコフ方策系列の集合"]
    D
  end
  subgraph D[定常な決定的マルコフ方策系列の集合]
  end
```

## 定式化

### 概観：逐次的意思決定問題

逐次的意思決定問題の学習においては方策 $\pi$ のみを調整します。環境（MDPの場合は $\{\mathcal{S,A},p_{s_0},p_T,g\}$ ）は一般に時間不変であり、最初に課題を設定した時点で決定されます。環境のモデルが既知である場合（モデルベース）はそれ自体から方策を最適化することが可能であり、このようなケースは**学習**（*learning*）の代わりに**プランニング**（*planning*）とよぶことも多いです。プランニングの場合に用いられる基礎的な最適化手法としては動的計画法や線形計画法などがあります。

一方環境のモデルが未知である場合はデータからの学習が必要です。バッチ学習の場合は与えられたデータを利用することのみを考えればよいですが、オンライン学習では局所最適解に陥ることを防ぐため**探索と活用のトレードオフ**（**exploration-exploitation trade-off*）を考慮しながらデータを収集することになります。

### MDPの表現の統一

対象とする逐次的意思決定問題の設定により、MDPの終了条件は

1. 状態が一定条件を満たしたら終了
2. 一定時間ステップの時点で終了
3. 終了しない（無限時間長のMDP）

などが考えられます。このうち、**吸収状態**（*absorbing state*）を設定することで条件 1,2 はいずれも条件 3 に統合することができます。ここで、吸収状態とは他の状態に遷移しない状態のことで、他の状態への遷移時報酬を $0$ とすることで実現できます。

> - 1→3 : 条件を満たした以降を吸収状態とする
> - 2→3 : ステップ数を状態として持たせ、規定時間以降を吸収状態とする

#### エルゴード性

MDPの状態の確率過程はマルコフ連鎖として捉えることが可能です。マルコフ連鎖は状態変数が離散的なマルコフ過程でしたが、この過程は**エルゴード性**（*ergodic property*）と呼ばれる性質を持っています。エルゴード性は既約性と非周期性からなり、マルコフ連鎖を $\mathrm{MC}(\pi)$ とすれば次のように説明できます[0]。

1. **既約的**（*irreducibility*）: $\mathrm{MC}(\pi)$ のすべての状態が互いに行き来可能<br>　 $\mathrm{Pr}(S_t=j|S_0=i,\mathrm{MC}(\pi))>0,\space\forall i,j\in\mathcal{S},\exist t\in\R$
2. **非周期的**（*aperiodicity*）: 任意の２状態間の推移について推移パターンの繰返しがない<br>　 $\gcd{\mathcal{T}(s)}=1\space s.t. \space \mathcal{T}(s)\triangleq\{t\geq1\colon\mathrm{Pr}(S_t=s|S_0=s)>0\}$

方策勾配法などではMDPのマルコフ連鎖が常にエルゴード性を満たすと仮定して議論を進めることが多いですが、上の条件1,2に基づくMDPは明らかに既約ではありません。したがって、これらのMDPは条件３のMDPとして拡張してもエルゴード性をもつことはなく、エルゴード性に基づいた議論（**定常分布**の存在や**ベルマン期待方程式**など）を利用することはできません。

### リターン（累積報酬）

逐次的意思決定問題では、一般に各時間ステップ $t$ から得られる報酬の和を取った確率変数、**リターン**について何らかの最大化を行うことで最適化を行います。多くの場合、リターンには以降のステップの単純な報酬和 $\sum_{k=0}^\infty{R_{t+k}}$ ではなく、

$$C_t\triangleq\sum_{k=0}^\infty{\gamma^k R_{t+k}}\tag{20}$$

を用いることが多いです。この式は**割引率**（*discounted rate*） $\gamma\in[0,1)$ による報酬和を取っていることから**割引累積報酬**（*discounted cumulative reward*）と呼ばれます。割引率 $\gamma$ は課題の目的ごとにあらかじめ設定される**ハイパーパラメタ**であり、0に近づけると短期的な報酬を、1に近づけると長期的な報酬を重要視して学習を行うようになります。

上の定義からリターンは**再帰的に**定義でき、

$$C_t = R_t+\gamma C_{t+1}\tag{21}$$

報酬が有界であることから（ $|R|\leq R_{max}$ ; 式 $(1)$ ）リターンも有界となります。

$$|C_t|\leq \sum_{k=0}^\infty{\gamma^k R_{max}}=\frac{R_{max}}{1-\gamma},\space\forall t\in\N_0$$

> $\N_0$ : $0$ を含む自然数

### 目的関数

上に述べたように、逐次的意思決定問題では最適化のためにリターンについて何らかの最大化を行います。ここで、方策 $\pmb{\pi}$ に基づくMDP $M(\pmb\pi)$ のもとで、統計量 $\mathcal{F}[C|M(\pmb\pi)]$ について

$$\mathcal{J}(\pmb\pi)\triangleq\mathcal{F}[C|M(\pmb\pi)]$$

を満たすような関数 $\mathcal{J}\colon\pmb{\Pi}\to\R$ を考えます。この関数を**目的関数**（*objective function*）といい、方策についての最適化問題はこの関数や制約条件の下で解かれることになります。制約条件なしの下では、逐次的意思決定問題は

$$\pmb{\pi}^*\triangleq\argmax_{\pmb{\pi}\in\Pi}{f(\pmb\pi)}$$

の探索問題と解釈できます。

> ここからわかるように、必ずしも最適方策は１つではなく、複数存在することがあります。

目的関数としては、ステップ全体を通したリターンの期待値

$$\mathcal{J}(\pmb{\pi})=\mathbb{E}^{\pmb\pi}[C_0]=\mathbb{E}^{\pmb\pi}\left[\sum_{t=0}^\infty{\gamma^t R_t}\right]\tag{23}$$

が用いられることが多いです。この目的関数は

> **価値関数**（*value function*） $V^{\pmb\pi}\colon\mathcal{S}\to\R$ を導入すると、式 $(23)$ の目的関数は
>
> $$\mathcal{J}(\pmb\pi)=\sum_{\pmb{s}\in\mathcal{S}}{p_{s_0}(s)V^{\pmb\pi}(s)}$$
>
> すなわち初期状態分布 $p_{s_0}$ による価値関数 $V^{\pmb\pi}(s)$ の重み付き和と解釈できます。ここで、価値関数は次で定義される、方策 $\pmb{\pi}$ の下での、初期状態を $s$ とした期待リターンです。
>
> $$V^{\pmb\pi}(s)\triangleq\mathbb{E}^{\pmb{\pi}}[C_0|S_0=s]$$

## 参考文献

[0][メイン] 強化学習, 森村哲郎, MLP 機械学習プロフェッショナルシリーズ, 講談社, 第一刷（Chapter 1）