# 準備

## はじめに

強化学習は機械学習の一種であり、報酬という概念が出てくるという点、その期待値を最大化するような逐次的意思決定ルールを学習することを目的とする点が特徴的です。ここで、逐次的意思決定ルールは**方策**（*policy*）と呼ばれます。また一般に、強化学習では**マルコフ性**という仮定をおきます。これらについて説明を行っていきます。

## 目次

- [準備](#準備)
  - [はじめに](#はじめに)
  - [目次](#目次)
  - [逐次意思決定問題](#逐次意思決定問題)
    - [マルコフ性（ok）](#マルコフ性ok)
    - [マルコフ決定過程（ok）](#マルコフ決定過程ok)
    - [目的関数（ok）](#目的関数ok)
  - [方策](#方策)
    - [方策の分類](#方策の分類)
- [参考文献](#参考文献)


## 逐次意思決定問題

強化学習が目的とする方策の最適化問題、すなわち**逐次意思決定問題**（*sequential decision-making problem*）を解くために必要な概念について学習していきます。

### マルコフ性（ok）

何らかの事象 $A$ の確率を $\mathrm{Pr}(A)$ と書くとしましょう。取りうる値とその値になる確率が定められている変数は**確率変数**と呼ばれ、実際に取った値は**実現値**と呼ばれます。理想的な６面さいころを例に取ってこれを説明すると、

$$\mathrm{Pr}(X=x)=\frac16,\space\forall x\in\mathcal{X}\triangleq\{1,2,3,4,5,6\}$$

と書けます。ここで、以下では確率変数を大文字（ $X$ ）で、実現値を小文字（ $x$ ）で、確率変数の取る値の集合をカリグラフ体（ $\mathcal X$ ）を用いて区別することににします。このような確率変数と確率との対応関係のことを**確率分布**（*probability distribution*）と呼びます。

さて、**マルコフ性**ですが、確率変数の並びを考えた場合に、将来の確率変数の条件付確率分布が、現時点のステップ（ $t$ ）にのみ依存する（すなわち $t-1$ 以前の値 $x_1,...,x_{t-1}$ には依存しない）という性質です。マルコフ性を持つ場合、それらは

$$\mathrm{Pr}(X_{t+k}=x|X_s=x_s,\forall s\leq t)=\mathrm{Pr}(X_{t+k}=x|X_t=x_t)$$

を満たします。ここで $t,k$ は任意の自然数であり、また $x_s,x\in\mathcal{X}$ です。

> 確率変数の並び：確率変数の系列のことであり、**確率過程**と呼ばれます。

確率変数 $X$ を状態変数とみなせば、 $\mathrm{Pr}(X_{t+1}=x'|X_t=x)$ は状態 $x$ が次のステップで $x'$ に遷移する確率を表すので、一般に**状態遷移確率**（*state transition probability*）と呼ばれます。また、マルコフ性を持つ確率過程を **マルコフ過程** といい、状態変数の取りうる値が離散的である場合には**マルコフ連鎖**と呼ばれます。

### マルコフ決定過程（ok）

強化学習においては何らかの選択や行動を行いますから、その**行動**（action）と、その良し悪しを判断させるための**報酬**（reward）をマルコフ連鎖に加えます。このような過程を**マルコフ決定過程**（Markov decision process; MDP）と呼び、次の組 $(\mathcal{S,A},p_{s_0},p_T,g)$ で定義します[0]。

|名称|定義|
|:-:|-|
|有限状態集合| $\mathcal{S}\triangleq\{s^1,...,s^{\|\mathcal{S}\|}\}\ni s$ |
|有限行動集合| $\mathcal{A}\triangleq\{a^1,...,a^{\|\mathcal{A}\|}\}\ni a$ |
|初期状態確率関数| $p_{s_0}\colon\mathcal{S}\to[0,1]\colon p_{s_0}(s)\triangleq\mathrm{Pr}(S_0=s)$ |
|状態遷移確率関数| $p_T\colon\mathcal{S\times S\times A}\to[0,1]\colon$ |
|〃| $p_T(s'\|s,a)\triangleq\mathrm{Pr}(S_{t+1}=s'\|S_t=s,A_t=a),\space\forall t\in\mathbb{N}_0$ |
|報酬関数| $g\colon\mathcal{S\times A}\to\mathbb{R}$ |

> $\mathbb{N}_0 \coloneqq \mathbb{N}\cup\bold{0}$ 、 $|\mathcal{X}|$ は $\mathcal X$ の要素数としました。

ここで、確率変数 $S_t$ と $A_t$ は時間ステップ $t\in\mathbb{N}_0$ での状態変数と行動変数を表します。上からわかるように、このMDPは「有限状態集合、有限行動集合の離散時間MDP」です。

定義から、報酬関数 $g$ は有界関数（ $\infty$ に飛ばない）であり、

$$|g(s,a)|\leq R_{max},\space\forall (s,a)\in\mathcal{S\times A}\tag 1$$

を満たす $R_{max}\in\mathbb R$ が存在することを仮定していることになります。また、報酬の集合 $\mathcal R$ を次のように定義します。

$$\mathcal{R}\triangleq\{r\in\mathbb{R}\colon r=g(s,a),\exist (s,a)\in\mathcal {S,A}\}$$

ここで、報酬にマイナスを掛けたものを**損失**（cost）と呼ぶことがありますが、累積損失の最小化を目的とした問題は累積報酬の最大化を目的とした問題と同様であることに注意したいです。

次に、MDPにおいて行動を選択する基準である**方策**（*policy*）を定義します。方策は関数であり、ここでは現ステップの状態 $s にのみ依存して確率的に行動を選択する**確率的方策**を

$$\pi\colon\mathcal{A\times S}\to[0,1]\colon\pi(a|s)\triangleq\mathrm{Pr}(A=a|S=s)\tag2$$

のように定義します。ここで、方策 $\pi$ を含めたMDP; $M$ を、

$$M(\pi)\triangleq\{\mathcal{S,A},p_{s_0},p_T,g,\pi\}\tag3$$

と表記することにします。また（すべての）方策を含む方策集合 $\Pi$ を次のように定義します。

$$\Pi\triangleq\left\{\pi\colon\mathcal{A\times S}\to[0,1]\colon\sum_{a\in\mathcal A}\pi(a|s)=1,\space\forall s\in\mathcal S\right\}$$

最後に、マルコフ決定過程 $M(\pi)$ がどのようにステップを進めるかについて下記に示します。

> マルコフ決定過程 $M(\pi)=\{\mathcal{S,A},p_{s_0},p_T,g,\pi\}$
>
> 0. 時間ステップ $t=0$ で初期化を行い、（初期状態確率 $p_{s_0} に従い）初期状態 $s_t\space(\sim p_{s_0})$ を決定する
> 1. 状態 $s_t$ と 方策 $\pi(\cdot|s_t)$ から、行動 $a_t$ を選択する
> 2. 行動 $a_t$ に対する報酬 $r_t=g(s_t,a_t)$ を受け取る
> 3. 現在の状態 $s_t$ と行動 $a_t$ から状態遷移確率 $p_T(\cdot|s_t,a_t)$ により、状態を次の $s_{t+1}$ へ遷移させる
> 4. 時間ステップを $t$ から $t+1$ に進め、1. に戻る

> $s_t\sim p_{s_0}$ は確率変数 $s_t$ が確率分布 $p_{s_0}$ に従うことを意味します。

### 目的関数（ok）

逐次意思決定問題では、与えられた方策集合から目的関数を最大にするような方策を探し出そうとします。ここで、目的関数は方策を評価する関数であり、例えば**期待報酬**（*expected reward*）

$$\mathbb{E}\left[\lim_{T\to\infty}{\frac1T\sum_{t=0}^{T-1}R_t}|M(\pi)\right]$$

であるとか、**期待リターン**（**expected return**）とよばれる**期待割引累積報酬**（*expected discounted cumulative reward*）

$$\mathbb{E}\left[\lim_{T\to\infty}{\sum_{t=0}^T\gamma^tR_t}|M(\pi)\right]$$

などであることが多いです。ここで、 $\gamma\in[0,1)$ は**割引率**（*discount rate*、または割引係数）とよばれ、長期的な報酬和をどの程度考慮するかを調整します。例えば $\gamma$ が小さい場合、選択される行動は将来の報酬よりも現在得られる報酬をより重視したものになります。

> $\mathbb{E}[X|Y]$ は、条件 $Y$ が与えられたときの確率変数 $X$ の期待値を表します

## 方策

上で述べたように、エージェントは方策に従い行動を決定します。この方策について、以下ではMDPに関するものを定義します。

> 一般的に機械学習の分野では、上記のような行動や選択を行う学習者のことを**エージェント**（*agent*）と呼びます。また、学習で扱う系全体のことは**環境**（*environment*）と呼びます。例として、家でハイハイを学習する赤ちゃんをこれに見立てれば、赤ちゃんはエージェントであり、その環境は家といえます。

### 方策の分類

式 $(2)$ で定義した確率的方策 $\pi$ の集合 $\Pi$ の部分集合として、**決定的方策**（*deterministic policy*） $\pi^d$ の集合 $\Pi^d$ を定義できます。

$$\Pi^d\triangleq\{\pi^d\colon\mathcal{S\to A}\}\tag5$$



# 参考文献

[0][メイン] 強化学習, 森村哲郎, MLP 機械学習プロフェッショナルシリーズ, 講談社, 第一刷（Chapter 1）