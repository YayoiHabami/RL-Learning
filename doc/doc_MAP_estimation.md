# なんか書いててなにがやりたいのかわからんくなってきた

とくにベイズの定理をこんなに長々と説明する必要あるか？

# MAP推定

MAP推定（Maximum a Posteriori estimation; 最大事後確率推定）をEMアルゴリズムにより行うことが可能です。

## ベイズの定理

### 概要：ベイズの定理

はじめにベイズ統計学やベイズ推論でよく使われるいくつかの語を記述します。ここで、 $X,Y$ を事象として定義し、簡単のため $Y$ を結果、 $X$ をその原因であると仮定します。

**事前確率**（*prior probability*） $P(X)$ は $X$ が起こる確率であり、結果 $Y$ に依存しない $X$ の起こる確率を示します。次に**事後確率**（*posterior probability*） $P(X|Y)$ は結果 $Y$ が生じた差異、その原因が $X$ である条件付き確率です。同様に条件付き確率 $P(Y|X)$ も考えられますが、「原因 $X$ が生じた際に結果 $Y$ が発生する確率」ではなく「結果 $Y$ が生じたとき、その原因が $X$ であることのもっともらしさ（尤もらしさ）」、すなわち**尤度**（*likelihood*）として一般に扱います。最後に、結果 $Y$ の確率 $P(Y)$ は**全確率**や**周辺尤度**と呼ばれますが、この詳細はのちに述べます。

さて、上記の確率（尤度）を用いると、**ベイズの定理**（*Bayes' theory*）が次のように定義できます。

$$P(X|Y)=\frac{P(Y|X)P(X)}{P(Y)}$$

この式は条件付き確率の定義 $P(X|Y)=\frac{P(X\cap Y)}{P(Y)}, P(Y|X)=\frac{P(X\cap Y)}{P(X)}$ から導出されます。

> ここで、ベイズの定理は**時間の流れに逆らった**確率の計算を行うための手法と考えることもできます[1]。上式の $X,Y$ をそれぞれ原因、結果として置き換えると
>
> $$\underbrace{P(原因|結果)}_{時間の流れと逆}=\frac{P(結果|原因)P(原因)}{P(結果)}$$
> 
> 右辺の $P(原因), P(結果)$ は時間の流れに関係のない計算で、また $P(結果|原因)$ も原因に基づく結果の確率ですので順方向の確率を計算しています。一方で左辺の $P(原因|結果)$ は「結果に基づく原因の確率」であり、ある種時間の流れに逆らっていると見ることもできます。ここから、ベイズの定理では時間の流れを逆転させる計算を行える、と考えることも可能ではないでしょうか。

さて先ほど省略した全確率（周辺尤度） $P(Y)$ ですが、 $X$ が原因、 $Y$ が結果という仮定のもとでは計算が大変です。というのも $P(Y)$ を考えるためには、すべての原因 $x$ について原因が $x$ であった場合の結果 $Y$ の生じる確率 $P(Y|X=x)P(X=x)$ を足し合わせる必要があるからです。たとえば原因 $X$ が取りうる事象が離散値であった場合、その値を $x_1,x_2,...,x_n$ とすれば全確率は次のように書くことができます。

$$P(Y)=\sum_{i=1}^n{P(Y|X=x_i)P(X=x_i)}$$

ここで、右辺では左辺にあった原因 $X$ が消えてなくなりました。このように事象（変数）の確率の和（積分）をとることを**周辺化**というため、全確率は**周辺尤度**（*marginal likelihood*）とも呼ばれます。

### 連続型確率分布への拡張

上に述べたベイズの定理では離散値の事象を対象として考えましたが、後のためには連続型の確率分布を考える必要があります。結果 $y$ のための統計モデルを考え、そのパラメタを $\theta$ とします。確率密度関数 $f$ を用いて、ベイズの定理は

$$f(\theta|y)=\frac{f(y|\theta)f(\theta)}{f(y)}=\frac{\mathcal{L}(\theta;y)f(\theta)}{f(y)}$$

と書けます。ここで $f(y|\theta)$ は尤度ですので、 $\mathcal{L}(\theta;y)$ とも置き換え可能です。また、このとき周辺尤度 $f(y)$ は次のように計算されます。

$$f(y)=\int_{-\infty}^\infty{f(y|\theta)f(\theta)d\theta}$$



##

最尤推定ではパラメタ $\theta$ を決定論的な変数として扱いました。これを確率変数として扱いたい場合もあると思います。この場合に用いられるのが**ベイズ推論**（*Bayesian inference*）です。

パラメタ $\theta$ を確率変数として扱うことにしたので、以下ではその分布を考えることになります。標本

## 最大事後確率（MAP）

> *maximum a posteriori*とは
>
> *a posteriori*は「より後のものから」を意味するラテン語です。条件付き確率の一種である事後確率はその英名を*posterior probability*ということから、*maximum a posteriori*は単純に最大の事後確率であることがわかります。

## 参考文献

[1] 入門統計学 第2版, 栗原伸一, オーム社, 第1刷

[2] 機械学習のための確率と統計, 杉山将, 講談社, 第1刷
