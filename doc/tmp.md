雑多なことを記述する。

## 面白そうな文献

- 現場で使える！ Python深層強化学習入門, 伊藤多一ほか, 翔泳社, 第一刷
  - 548.13 G75
  - 実装という面に重きが置かれているため、その前提となるシステムの説明もわかりやすいものが多い。実装対象はDQNとかActor-Criticとかではあるが、基礎知識の学習などに便利そうだと感じた
- Pythonで学ぶ強化学習 第二版, 久保宏隆, 
  - 548.13 Ku11
  - 結構いろいろなアルゴリズムの実装がのっている感じがある。Q学習とかはもちろんA2Cとかも。

## 草稿

> $$\mathcal J(\pi)\coloneqq\mathbb{E}_\xi[\sum_{t=0}^T \gamma^t r(\bm s_t)]\tag{1}$$
>
> 目的関数 $\mathcal J(\pi)$ は、ポリシー $\pi$ の場合の**割引報酬**の**期待値**を計算します。詳細は以下の通りです。
>
> わたしたちは最適なポリシー $\pi$ （環境⇒行動の道筋）を探すことで、目的関数 $\mathcal J(\pi)$ を最大化したいです。ここで、目的関数は現在のポリシー $\pi$ に従い行動した際の累積報酬をもとにした、そのポリシーの**長期的な**パフォーマンスを意味します。
>
> 期待値 $\mathbb E_\xi$ は初期状態の分布 $\xi$ にわたる割引収益の平均値です。 これは、考えられるすべての開始状態とその確率を考慮し、ポリシー $\pi$ に基づいて各状態の割引収益の平均値を計算することを意味します。
> 
> 割引収益 $\sum_{t=0}^T \gamma^t r(\bm s_t)$ は、各タイム ステップ $t$ で得られる報酬 $r(\bm s_t)$ の合計に a を乗じたものです。 割引係数 $\gamma^t$。 割引係数 $\gamma$ は、当面の報酬と比較して将来の報酬をどの程度評価するかを決定する 0 から 1 までの数値です。 $\gamma$ が高いほど、将来の報酬をより重視することを意味し、$\gamma$ が低いほど、目先の報酬を好むことを意味します。 また、割引係数により、たとえホライズン $T$ が無限であっても、割引された報酬の合計が有限であることが保証されます。
> 
> 報酬 $r(\bm s_t)$ は各状態 $\bm s_t$ に数値を割り当てる関数です。 これは、いくつかの基準に従って各状態がどの程度望ましいか望ましくないかを反映します。 たとえば、ゲームでは、報酬は勝利の場合はプラス、負けの場合はマイナス、その他の状態の場合はゼロになる可能性があります。
>
> 式の期待値を計算するには、初期状態の分布 $\xi$、ポリシー $\pi$、報酬関数 $r$、および割引係数 $\gamma$ を知る必要があります。 次に、次の式を使用できます。
>
> $$\mathcal J(\pi) = \sum_{\bm s_0} \xi(\bm s_0) \sum_{t=0}^T \gamma^t r(\bm s_t)$$
> 
> ここで、$\bm s_t$ は、初期状態 $\bm s_0$ からポリシー $\pi$ に従ってタイム ステップ $t$ に到達した状態です。 この式は、問題の特性と仮定に応じて、動的計画法、モンテカルロ法、または時間差分学習を使用して計算できます。

## POMDP&MPOによる環境での、エージェントのフロー

1) 上記の設定でエージェントの行動が最適化される一般的な流れは以下の通りです。

- エージェントは、観測値から行動へのマッピングを行う初期方策$\pi_0$から始めます。
- エージェントは現在の方策に従って環境と相互作用し、観測値、行動、報酬、次の観測値の軌跡を収集します。
- エージェントは収集した軌跡を用いて、現在の方策の下での状態-行動価値関数$Q(s, a)$と状態分布$\xi(s)$を推定します。
- エージェントはMPOの最適化問題を解きます。これは、現在の方策$\pi_t(a|s)$に対するKLダイバージェンス制約の下で、$Q(s, a)$の期待値を最大化するような新しい行動分布$q(a|s)$を見つけるという問題です。
- エージェントは方策$\pi_{t+1}$を更新します。これは、最適な行動分布$q(a|s)$に等しく設定します。
- エージェントは収束するか、終了条件が満たされるまで、2-5のステップを繰り返します。

2) 物理エンジンによる計算は二つの段階で行われます。

- エージェントが環境と相互作用するとき、物理エンジンは環境とエージェントの行動の動力学をシミュレートし、エージェントに観測値と報酬を生成します。
- エージェントがMPOの最適化問題を解くとき、物理エンジンは目的関数と制約の勾配を行動分布$q(a|s)$に関して計算します。これは物理エンジンが微分可能であり、シミュレーションを通して勾配を伝播させることができるからです。

#### 補足：ノンパラメトリック

**ノンパラメトリック**は分布が固定されたり既知の形やパラメタを持たないことを意味します。分布は事前に定義されたモデルでは**なく**、データに基づいています。逆に**パラメトリック**はパラメタに基づくデータであり、**特定の分布を背後に想定している**ことを意味します。


